import torch

# 声明初始值
x = torch.Tensor([0])
x.requires_grad_(True)

# 自己定义一个函数，假装是loss函数
y = x ** 2 + 2 * x + 1

# 定义学习率
learning_rate = torch.Tensor([0.1])

#定义迭代次数
epoches = 15 


for epoche in epoches:
    # 前向传播算loss
    y = x ** 2 + 2 * x + 1
    # 反向传播求梯度
    y.backward()
    print('x=', x.data, 'y=', y.data)

    x.data = x.data - learning_rate * x.grad.data
    # 注意：在PyTorch 中梯度如果不清零就会积累（（因为PyTorch是基于动态图的， 每迭代一次就会留下计算缓存，到下一次循环时需要手动清除缓存））
    x.grad.data.zero_()

