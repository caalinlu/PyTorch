{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caalinlu/PyTorch/blob/master/lesson2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubJp9ZDjK2IC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "ef41b3ea-a74f-45ac-ef6b-ab1b449d2004"
      },
      "source": [
        "# Pytorch实现梯度下降\n",
        "import torch\n",
        "\n",
        "# 声明初始值\n",
        "x = torch.Tensor([0])\n",
        "x.requires_grad_(True)\n",
        "\n",
        "# 自己随便定义一个函数，假装是loss函数\n",
        "y = x ** 2 + 2 * x + 1\n",
        "\n",
        "# 定义学习率\n",
        "learning_rate = torch.Tensor([0.1])\n",
        "\n",
        "# 定义迭代次数\n",
        "epoches = 15 \n",
        "\n",
        "\n",
        "for epoche in range(epoches):\n",
        "    # 前向传播算loss\n",
        "    y = x ** 2 + 2 * x + 1\n",
        "    # 反向传播求梯度\n",
        "    y.backward()\n",
        "    print('x=', x.data, 'y=', y.data)\n",
        "\n",
        "    x.data = x.data - learning_rate * x.grad.data\n",
        "    # 注意：在PyTorch 中梯度如果不清零就会积累（（因为PyTorch是基于动态图的， 每迭代一次就会留下计算缓存，到下一次循环时需要手动清除缓存））\n",
        "    x.grad.data.zero_()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x= tensor([0.]) y= tensor([1.])\n",
            "x= tensor([-0.2000]) y= tensor([0.6400])\n",
            "x= tensor([-0.3600]) y= tensor([0.4096])\n",
            "x= tensor([-0.4880]) y= tensor([0.2621])\n",
            "x= tensor([-0.5904]) y= tensor([0.1678])\n",
            "x= tensor([-0.6723]) y= tensor([0.1074])\n",
            "x= tensor([-0.7379]) y= tensor([0.0687])\n",
            "x= tensor([-0.7903]) y= tensor([0.0440])\n",
            "x= tensor([-0.8322]) y= tensor([0.0281])\n",
            "x= tensor([-0.8658]) y= tensor([0.0180])\n",
            "x= tensor([-0.8926]) y= tensor([0.0115])\n",
            "x= tensor([-0.9141]) y= tensor([0.0074])\n",
            "x= tensor([-0.9313]) y= tensor([0.0047])\n",
            "x= tensor([-0.9450]) y= tensor([0.0030])\n",
            "x= tensor([-0.9560]) y= tensor([0.0019])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iQMYTB2EqaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch实现线性回归\n",
        "import torch\n",
        "# print(torch.__version__)\n",
        "\n",
        "# train data\n",
        "x_data = torch.arange(1.0,4.0,1.0)\n",
        "\n",
        "# Numpy中的size是元素个数，但是在Pytorch中size等价为Numpy中的shape\n",
        "# view 可以视作调整数组的size， -1 的作用是用来自动计算在列数确定的情况下行数\n",
        "x_data = x_data.view(-1, 1)\n",
        "y_data = torch.arange(2.0, 7.0, 2.0)\n",
        "y_data = y_data.view(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb9r5NSZG-rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 超参数设置\n",
        "learning_rate = 0.1\n",
        "num_epoches = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JPWaGhVHJ0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegression(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = torch.nn.Linear(1, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    y_pred = self.linear(x)\n",
        "    return y_pred\n",
        "  \n",
        "model = LinearRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3awwus05H219",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "outputId": "b87e7709-88af-4248-9c1d-293adcf2da6d"
      },
      "source": [
        "\n",
        "# 定义loss function损失函数\n",
        "# PyTorch0.4以后，使用reduction参数控制损失函数的输出行为\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "# nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(num_epoches):\n",
        "  \n",
        "  # forward\n",
        "  y_pred = model(x_data)\n",
        "  \n",
        "  # computing loss \n",
        "  loss = criterion(y_pred, y_data)\n",
        "  print(epoch,'epoch\\'s loss:',loss.item())\n",
        "  \n",
        "  # backward:zero gradients + backward + step\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() # 执行梯度下降\n",
        "  \n",
        " # testing\n",
        "x_test=torch.Tensor([4.0])\n",
        "print(\"the result of y when x is 4:\",model(x_test))\n",
        "print('model.parameter:',list(model.parameters()))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 epoch's loss: 16.615921020507812\n",
            "1 epoch's loss: 0.5019358992576599\n",
            "2 epoch's loss: 0.29503700137138367\n",
            "3 epoch's loss: 0.2788357734680176\n",
            "4 epoch's loss: 0.26556479930877686\n",
            "5 epoch's loss: 0.25294986367225647\n",
            "6 epoch's loss: 0.24093462526798248\n",
            "7 epoch's loss: 0.22949011623859406\n",
            "8 epoch's loss: 0.21858921647071838\n",
            "9 epoch's loss: 0.20820589363574982\n",
            "10 epoch's loss: 0.1983160823583603\n",
            "11 epoch's loss: 0.18889588117599487\n",
            "12 epoch's loss: 0.1799231767654419\n",
            "13 epoch's loss: 0.17137672007083893\n",
            "14 epoch's loss: 0.16323624551296234\n",
            "15 epoch's loss: 0.1554822474718094\n",
            "16 epoch's loss: 0.14809675514698029\n",
            "17 epoch's loss: 0.14106221497058868\n",
            "18 epoch's loss: 0.1343616247177124\n",
            "19 epoch's loss: 0.12797926366329193\n",
            "20 epoch's loss: 0.12190017849206924\n",
            "21 epoch's loss: 0.11610981822013855\n",
            "22 epoch's loss: 0.1105945035815239\n",
            "23 epoch's loss: 0.10534120351076126\n",
            "24 epoch's loss: 0.10033739358186722\n",
            "25 epoch's loss: 0.09557127952575684\n",
            "26 epoch's loss: 0.09103161841630936\n",
            "27 epoch's loss: 0.08670753240585327\n",
            "28 epoch's loss: 0.08258878439664841\n",
            "29 epoch's loss: 0.07866580039262772\n",
            "30 epoch's loss: 0.07492917776107788\n",
            "31 epoch's loss: 0.07136993110179901\n",
            "32 epoch's loss: 0.06797981262207031\n",
            "33 epoch's loss: 0.06475075334310532\n",
            "34 epoch's loss: 0.06167503446340561\n",
            "35 epoch's loss: 0.058745432645082474\n",
            "36 epoch's loss: 0.0559549555182457\n",
            "37 epoch's loss: 0.05329711735248566\n",
            "38 epoch's loss: 0.05076548829674721\n",
            "39 epoch's loss: 0.048354025930166245\n",
            "the result of y when x is 4: tensor([7.5696], grad_fn=<AddBackward0>)\n",
            "model.parameter: [Parameter containing:\n",
            "tensor([[1.7507]], requires_grad=True), Parameter containing:\n",
            "tensor([0.5666], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
