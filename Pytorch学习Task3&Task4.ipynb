{"cells":[{"metadata":{"id":"1E31E192E2F049709E7C182931A0ADD1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","source":"# 去重,建立字符到索引的映射idx_to_char\nidx_to_char = list(set(corpus_chars))\n# 建立索引到字符的映射corpus_indices\nchar_to_idx = {idx:char for char, idx in enumerate(idx_to_char)}\nvocab_size = len(char_to_idx)\nprint(vocab_size)\n\ncorpus_indices = [char_to_idx[char]  for char in corpus_chars]\nsample = corpus_indices[:20]\nprint('chars:', ''.join([idx_to_char[idx] for idx in sample]))\nprint('indices:', sample)","execution_count":24,"outputs":[]},{"metadata":{"id":"B2968CCACEC14A0D8571D8945472F629","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 时间序列的采样\n对于时间序列的采样，进行的操作是对于一句话，选取一定的时间步数，之后再句子中选取连续的时间步数的词组char作为训练集的样本（也就是下面的X），而其对应的char向后退移动一步作为训练样本对应的label（也就是下面的Y）。\n举个例子：\n现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：\n* $X$：“想要有直升”，$Y$：“要有直升机”\n* $X$：“要有直升机”，$Y$：“有直升机，”\n* $X$：“有直升机，”，$Y$：“直升机，想”\n* ...\n* $X$：“要和你飞到”，$Y$：“和你飞到宇”\n* $X$：“和你飞到宇”，$Y$：“你飞到宇宙”\n* $X$：“你飞到宇宙”，$Y$：“飞到宇宙去”\n可以看到，如果序列的长度为T，时间步数为n，那么一共有T-n个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。"},{"metadata":{"id":"1C7E072A44414EA28913EBE0991C380A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 随机采样\n下面的代码每次从数据里随机采样一个小批量。其中批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。\n在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。**即先切片再随机抽样。**"},{"metadata":{"id":"752F08FAC2444E21AC65BDDAC98A14B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch\nimport random\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n-1个字符\n    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n    # 得到可取到每个时间步数的字符组的下标\n    example_indices = [i * batch_size for i in range(num_examples)]\n    random.shuffle(example_indices)\n    \n    def _data(i):\n        return corpus_indices[i: i+num_steps]\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n    for i in range(0, num_examples, batch_size):\n        # 每次选出batch_size个样本\n        batch_size_indices = example_indices[i:i+batch_size]\n        X = [_data(j) for j in batch_size_indices]\n        Y = [_data(j+1) for j in batch_size_indices]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n        ","execution_count":29},{"metadata":{"id":"C255FF5681AC4FCE85F2EB207640A266","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":true,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[ 6,  7,  8,  9, 10, 11],\n        [ 2,  3,  4,  5,  6,  7]]) \nY: tensor([[ 7,  8,  9, 10, 11, 12],\n        [ 3,  4,  5,  6,  7,  8]]) \n\nX:  tensor([[4, 5, 6, 7, 8, 9],\n        [0, 1, 2, 3, 4, 5]]) \nY: tensor([[ 5,  6,  7,  8,  9, 10],\n        [ 1,  2,  3,  4,  5,  6]]) \n\n","name":"stdout"}],"source":"my_seq = list(range(30))\nfor X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":30},{"metadata":{"id":"D1DAAC2B672C4DBB9D097DB3A50ADE65","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 相邻采样\n在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。**即当前batch是上一个batch是相邻样本，先切分，后按顺序采样。**"},{"metadata":{"id":"FC13B8AACE9044388048C8D996B67023","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    corpus_len = len(corpus_indices) // batch_size * batch_size\n    corpus_indices = corpus_indices[:corpus_len]\n    indices = torch.tensor(corpus_indices, device=device)\n    # resize成batch_size行\n    indices = indices.view(batch_size, -1)\n    batch_num = (indices.shape[1] - 1) // num_steps\n    for i in range(batch_num):\n        i = i * num_steps\n        # 行全取，每次取num_steps列\n        X = indices[:,i:i+num_steps]\n        Y = indices[:,i+1:i+num_steps+1]\n        yield X, Y","execution_count":31},{"metadata":{"id":"25A66EF35CE3498386C6D3D702D90757","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 循环神经网络\n\n## 循环神经网络的构造\n**我们先看循环神经网络的具体构造。**\n\t\n\ti.首先网络同样是有三层神经网络构成，输入是n个时态的输入值记为Xt。\n\tii.其次是隐藏层，隐藏层的输出记为Ht。\n\tiii.最后是输出层，输出层是不同时间位置对应的Ot\n\n**RNN的前向传播**\n用公式来表示一个最简单的循环神经网络前向传播的方法如下：\n$$\n\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).\n$$\n$$\n\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.\n$$\n以上公式表示的是循环神经网络的前向传播的公式。\n\n**RNN的反向传播**\n对于循环神经网络来说，其求取反向传播的过程与正常的DNN有一些区别。\n具体求导的过程可以参考[知乎连接RNN求导方式](https://zhuanlan.zhihu.com/p/32930648)\nRNN最终的损失是所有时刻的损失的和。\nRNN在反向传播的过程的时候，某时刻t的梯度和前面所有时刻的梯度都有关。\n\n**RNN反向传播的过程中为什么容易出现梯度爆炸或是梯度消失**\n这是RNN最大的缺陷，由于这一缺陷，使得RNN在长文本中难以训练。\n梯度消失指的是在反向传播梯度的时候，出现了梯度消失的现象，梯度向后传播的时候越乘越小，使得权重无法更新，导致训练失败。而梯度爆炸所带来的的问题就是梯度过大，梯度在向前传播的过程中，越变越大，从而大幅度更新网络参数，造成网络不稳定。在极端情况下，权重的值变得特别大，一直与结果会溢出。\t\n总之，无论是梯度消失还是梯度爆炸，都是源于网络结构太深，造成网络权重不稳定，从本质上来讲是\n因为梯度反向传播中的**连乘效应**。当句子特别长的时候，也就是状态很多的时候，梯度需要连乘的项就越来越多。而对于使用tanh作为激活激活函数的RNN。每次本次状态对前一次状态求导实际上都是进行一次tanh求导，当Wx/或者Ws来说，当这个值本身就很小时，梯度就会越来越小，从而出现梯度消失。而当Wx或者Ws很大的时候，梯度将会越乘越大，出现梯度爆炸的情况。\n\n\n"},{"metadata":{"id":"CB1B4AE9680D469C8C36195BEED60DEC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 从零开始实现RNN"},{"metadata":{"id":"D3BD6B2F17E44B2C869862723C63561B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch\nimport torch.nn as nn\nimport time\nimport math\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2l_jay9460 as d2l\n# 歌词字符集、字符索引映射列表、索引字符映射列表、歌词词库大小\n(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":1},{"metadata":{"id":"4D7DEC8B67C749B282283599211F70CB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])\ntorch.Size([2, 1027])\ntensor([1., 1.])\n","name":"stdout"}],"source":"# one_hot 编码， 将词库中的词索引转换为无数量比较的one_hot编码\ndef one_hot(x, n_class, dtype=torch.float32):\n    \"\"\"\n        x:输入的词组对应的数字索引（确切的说是连续的词组）\n        n_class：词库的数量\n    \"\"\"\n    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)\n    # scatter_(input, dim, index, src) → Tensor\n    # 将src中的所有值按照index确定的索引写入本tensor中。\n    # 其中索引是根据给定的dimension，dim按照gather()描述的规则来确定。\n    result.scatter_(1, x.long().view(-1, 1), 1)\n    return result\n\nx = torch.tensor([0, 2])\nx_tensor_hot = one_hot(x, vocab_size)\nprint(x_tensor_hot)\nprint(x_tensor_hot.shape)\nprint(x_tensor_hot.sum(axis=1))\n    ","execution_count":6},{"metadata":{"id":"D1A5B34DA4944F2F8844397DEEB428D3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"5 torch.Size([2, 1027])\n","name":"stdout"}],"source":"# 每次采样的小批量的形状是（批量大小， 时间步数）。\n# 下面的函数将这样的小批量变换成数个形状为（批量大小，词典大小）的矩阵，矩阵个数等于时间步数。\n# 输入的X行代表的意义是batch_size的数量，X列代表的含义是index个时刻\n# 每次取出batch_size个T时刻的词对应的词向量。\ndef to_onehot(X, n_class):\n    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n\nX = torch.arange(10).view(2, 5)\ninputs = to_onehot(X, vocab_size)\nprint(len(inputs), inputs[0].shape)","execution_count":7},{"metadata":{"id":"4E9C2628374247EE8F19A3905232CAA6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 初始化模型参数"},{"metadata":{"id":"F175C5A57EDE47B081320A32C312D9A9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 首先确定网络结构\n# 三层结构，参数为Wx,Ws,W0,b1,b2，分别代表输入值隐藏层，前一隐藏状态至当前隐藏状态，隐藏层到输出层\ndef get_params():\n    def _one(shape):\n        param = torch.zeros(shape, device=device, dtype=torch.float32)\n        nn.init.normal_(param, 0, 0.01)\n        return torch.nn.Parameter(param)\n    # 隐藏层参数\n    W_xh = _one((num_inputs, num_hiddens))\n    W_hh = _one((num_hiddens, num_hiddens))\n    b_h = torch.nn.Parameter(torch.zeros(niddens, device=device))\n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))\n    \n    return W_xh, W_hh, b_h, W_hq, b_q","execution_count":8},{"metadata":{"id":"4A787AB2171E4FC28EDC0C171FEC14B0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 定义模型\n函数rnn用循环的方式一次完成循环神经网络每个时间步的计算"},{"metadata":{"id":"A1F3B921AB6043508FDE0CD6BF1B81E8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def rnn(inputs, state, params):\n    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n    W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    # inputs是不同的时态*（batch_size,vocab_size）list\n    for X in inputs:\n        H = torch.tanh(torch.matul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H,)","execution_count":10},{"metadata":{"id":"089307B5E9244063BC12A297F9079A4E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def init_rnn_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), )","execution_count":11},{"metadata":{"id":"302931C18BB24600883FF12467F80D73","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# TODO\n最近导师开始催促完成项目\n之后再补上"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}