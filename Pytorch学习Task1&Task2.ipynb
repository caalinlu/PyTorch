{"cells":[{"outputs":[{"output_type":"stream","text":"houseprices2807\r\n","name":"stdout"}],"execution_count":1,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"65D7B6C0D67C4146A7C8F47FC997674F","scrolled":false}},{"outputs":[{"output_type":"stream","text":"lost+found\r\n","name":"stdout"}],"execution_count":2,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FD48405664C84234BE1E7D124AEB6D7C","scrolled":false}},{"metadata":{"id":"3DC2EF2F196E43B78B92D89ED187949E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 线性回归模型从零开始的实现\n## preface:\n两个向量直接做矢量加法比两个向量使用for循环按元素做标量加法要快的多"},{"metadata":{"id":"9146713720DE4D9F846D839AE57FC1CF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\nprint(torch.__version__)","execution_count":2},{"metadata":{"id":"0211B68651C742C59496AFB37042E9CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 首先生成线性回归的数据集"},{"metadata":{"id":"C8462F3CC8C540D394CE75BEB18DD4CA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 输入数据特征维数\nnums_inputs_dismention = 2\n# 输入数据集样本数量\nnums_example = 1000\n\n# 设定真实的w与b\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\n# 生成数据 生成2*1000的tensor\nfeatures = torch.randn(nums_example, nums_inputs_dismention, dtype=torch.float32)\nlabels = true_w[0] * features[:,0] + true_w[1] * features[:,1] + true_b\n\n# 对label引入噪音\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                        dtype=torch.float32)","execution_count":3},{"metadata":{"id":"966DE80C1EC94B82AD61690B46C821CE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/966DE80C1EC94B82AD61690B46C821CE/q5lb9a86f6.png\">"},"transient":{}}],"source":"# 用matplotlib画出上点的散点图\nplt.scatter(features[:,1].numpy(), labels.numpy(), 1);","execution_count":12},{"metadata":{"id":"1C65D6822B114A39879B33614E1DEC8D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 理解pytoch中的dataloader原理"},{"metadata":{"id":"FD9A2867AD46409B83F261D14A724AE4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 加载数据函数data_iter\ndef data_iter(batch_size, features, labels):\n    # pytorch中的tensor的len方法返回shape的第一个参数\n    nums_sample = len(features)\n    indices = list(range(nums_sample))\n    random.shuffle(indices)\n    for i in range(0, nums_sample, batch_size):\n        j = torch.LongTensor(indices[i:min(i+batch_size, nums_sample)])\n        # index_select()方法只接受一维的向量,第一个参数为维度\n        yield features.index_select(0, j), labels.index_select(0, j)","execution_count":4},{"metadata":{"id":"17CE46366C384FEEADD1EA178C27F550","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[-0.4694, -1.5969],\n        [ 0.6177,  0.9248],\n        [ 0.5586, -0.2254],\n        [ 0.9796,  0.1993],\n        [-1.8313, -0.0212],\n        [ 0.1410, -1.4526],\n        [ 2.4443,  1.1232],\n        [-0.2030, -0.2042],\n        [ 0.4109, -0.6639],\n        [-0.0164, -0.2261]]) tensor([8.6919, 2.2956, 6.0797, 5.4727, 0.6074, 9.4268, 5.2724, 4.4929, 7.2700,\n        4.9433])\ntorch.Size([10])\n","name":"stdout"}],"source":"# 测试函数\nbatch_size = 10\n\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, y)\n    print(y.size())\n    # 停止迭代\n    break\n","execution_count":7},{"metadata":{"id":"C9B81F570CBA4503928C4FCE9CA70E83","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 初始化模型参数"},{"metadata":{"id":"810F02FB164F4283AF8599E18EFBEED2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor(1., requires_grad=True)"},"transient":{},"execution_count":43}],"source":"w = torch.tensor(np.random.normal(0, 0.01, (nums_inputs_dismention, 1)), dtype=torch.float32)\nb = torch.tensor(1, dtype=torch.float32)\n\n# 标注需要求导的参数\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)","execution_count":43},{"metadata":{"id":"660ABB078B1F4CB783B8C407524A722E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义模型"},{"metadata":{"id":"C8844A31159A451D809199D32021DF7B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 定义用来训练参数的模型 y = w[0] * features[0] + w[1] * features[1] + b\ndef linreg(X, w, b):\n    # 用到torch中的mul\n    return  torch.mm(X, w) + b","execution_count":38},{"metadata":{"id":"A42FD521ED8B42518EA30C62458E3BBB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义损失函数"},{"metadata":{"id":"64DAB527732943C782444B0BE871AD1B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 均方函数SSE loss = 1/2 * ((y-y*)**2)\n# torch 中的view方法相当于numpy中的resize方法，用来调整tensor的大小\n# 把原先tensor中的数据按照行优先的顺序排成一个一维的数据（这里应该是因为要求地址是连续存储的），\n# 然后按照参数组合成其他维度的tensor。比如说是不管你原先的数据是[[[1,2,3],[4,5,6]]]还是[1,2,3,4,5,6]，\n# 因为它们排成一维向量都是6个元素，所以只要view后面的参数一致，得到的结果都是一样的。\n# 原文链接：https://blog.csdn.net/york1996/article/details/81949843\ndef squared_loss(y_hat, y):\n    return (y_hat-y.view(y_hat.size())) ** 2 / 2","execution_count":35},{"metadata":{"id":"7DB7A67CDA284C9D8A450C0333E8F5B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 定义优化函数"},{"metadata":{"id":"ECC6C6E722EF426E8D555E84BA130D5D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 自己定义优化函数时利用了pytorch的求导操作\ndef minibatch_sgd(params, lr, batch_size):\n    for param in params:\n        param.data -= lr * param.grad / batch_size","execution_count":47},{"metadata":{"id":"3BF7A329541747A79D179C92FB027D9D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 训练"},{"metadata":{"id":"FB60850EF95A4D29B7AB759C77543AC1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 1, loss 0.027933\nepoch 2, loss 0.000124\nepoch 3, loss 0.000052\nepoch 4, loss 0.000052\nepoch 5, loss 0.000052\n","name":"stdout"}],"source":"# 定义参数\nlr = 0.03\nnum_epoch = 5\ntotal_loss = 0.0\n\n\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epoch):\n    \n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y).sum()\n        # 反向传播求导，结果保存在tensor里\n        l.backward() \n        minibatch_sgd([w, b], lr, batch_size)\n        # 每次算完梯度后清零，防止梯度累加\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n        \n    train_l = loss(net(features, w, b), labels)\n    print('epoch %d, loss %f'% (epoch + 1, train_l.mean().item()))\n    \n        ","execution_count":48},{"metadata":{"id":"FF53EC93F2834D6B944A8A4C590B6785","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(tensor([[ 1.9997],\n         [-3.4002]], requires_grad=True),\n tensor(4.1998, requires_grad=True),\n [2, -3.4],\n 4.2)"},"transient":{},"execution_count":50}],"source":"w, b,  true_w, true_b","execution_count":50},{"metadata":{"id":"26D56B4C3D664B728F94FD1BA384F961","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 线性回归用pytorch实现"},{"metadata":{"id":"DA961824E7984B3F874CB3A78A01955A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"import torch\nfrom torch import nn\nimport numpy as np\n\ntorch.manual_seed(1)\nprint(torch.__version__)\ntorch.set_default_tensor_type('torch.FloatTensor')","execution_count":9},{"metadata":{"id":"14387A453F5B4F0E838B5362CF27DA23","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 同样的新建数据集\ntrue_w = [2, -3.4]\ntrue_b = 4.2\n\nnum_input = 2\nnum_sample = 1000\n\nfeatures = torch.tensor(np.random.normal(0, 1, (num_sample, num_input)), dtype=torch.float)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float32)","execution_count":10},{"metadata":{"id":"245F5D57892F43A4AB69355BE2A13097","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 定义数据读取\nimport torch.utils.data as Data\n\nbatch_size = 10\n\n# 需要将原始数据转换为tensor\ndataset = Data.TensorDataset(features, labels)\n\ndata_iter = Data.DataLoader(\n    dataset=dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2,\n    )","execution_count":11},{"metadata":{"id":"2B5CB2AE6ADC4E03B32EE3A882D7769B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"LinearNet(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\n","name":"stdout"}],"source":"# 定义网络\nclass LinearNet(nn.Module):\n    def __init__(self, n_features):\n        super(LinearNet, self).__init__() # 调用网络父类进行初始化\n        # 定义不同的层,线性层参数为输入输出的维数\n        self.linear = nn.Linear(n_features, 1)\n    def forward(self, x):\n        # 需要写出前向传播的过程\n        y = self.linear(x)\n        return y\n\nnet = LinearNet(num_input)\nprint(net)","execution_count":12},{"metadata":{"id":"041791FBB96B4AC986F443E1E857E692","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Sequential(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\nLinear(in_features=2, out_features=1, bias=True)\n","name":"stdout"}],"source":"# 更简单的方法建立网络模型\n# 方法一\nnet = nn.Sequential(\n    nn.Linear(num_input, 1))\n#方法二\nnet = nn.Sequential()\nnet.add_module('linear', nn.Linear(num_input, 1))\n# 方法三\nfrom collections import OrderedDict\nnet = nn.Sequential(OrderedDict([('linear', nn.Linear(num_input,1))]))\n\nprint(net)\n# 取出某一层\nprint(net[0])","execution_count":13},{"metadata":{"id":"7CCDD33D11774F4D8EE1F63BAF69024D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 定义损失函数\nloss = nn.MSELoss()","execution_count":14},{"metadata":{"id":"ECBB4CE5BBE34F109E7CD4DEDD332C99","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.03\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n","name":"stdout"}],"source":"import torch.optim as optim\n# 定义优化函数\noptimizer = optim.SGD(net.parameters(), lr=0.03)\nprint(optimizer)","execution_count":15},{"metadata":{"id":"73D53C1CF6304A2B9450EA79FC775E4D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"Parameter containing:\ntensor([0.], requires_grad=True)"},"transient":{},"execution_count":16}],"source":"# 初始化模型参数\nfrom torch.nn import init\ninit.normal_(net[0].weight, mean=0.0, std=0.01)\ninit.constant_(net[0].bias, val=0.0)","execution_count":16},{"metadata":{"id":"A7794A3933AB4B858DEB272F8BD6F7F8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":true,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\nepoch 0, loss: 0.000072\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\nepoch 1, loss: 0.000113\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\ntorch.Size([10, 1])\nepoch 2, loss: 0.000040\n","name":"stdout"}],"source":"# 训练\n\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    for X, y in data_iter:\n        output = net(X)\n        # 在定义网络的时候一般得到的运算结果都是[batch_size,1]的结果，对于y需要进行调整\n        l = loss(output, y.view(-1, 1))\n        optimizer.zero_grad() # 梯度清零\n        l.backward()\n        optimizer.step()\n    print('epoch %d, loss: %f'%(epoch, l.item()))","execution_count":18},{"metadata":{"id":"1FA67EF1F1EB4F3488FCB55482D40E69","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"tensor([[ 1.9991, -3.3999]]) [2, -3.4]\ntensor([4.2001]) 4.2\n","name":"stdout"}],"source":"dense = net[0]\nprint(dense.weight.data, true_w)\nprint(dense.bias.data, true_b)","execution_count":13},{"metadata":{"id":"DC8E2436260D4B41BC7C4348518374C4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# SoftMax和分类模型"},{"metadata":{"id":"82BC08128B244609813986C9EF332793","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Softmax的基本概念\n* **分类问题**\n\n* **权重矢量**\n* **神经网络图**\n  SoftMax通常是网络的最后一层，其是一个单层的神经网络，是一个全连接层。\n  对于分类问题最简单的办法是将输出值当做预测类别为“是”的置信度，并将值最大输出所对应的类作为预测输出，即输出。\n* **输出问题**\n\t直接使用上述单层神经网络的输出有两个问题。\n\t\ti.一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义（希望能够提供更加直观的概率的结果来表示每个类为‘是’的概率）\n\t\tii.另外，由于真实的标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量，这些离散值与不确定范围的输出之间的误差难以衡量\n\tsoftmax的计算方式其实就是将输出层的值经过指数级的缩放后再计算每个输出类的概率。\n\tsoftmax与lr的区别在于：\n\t\ti.softmax是进行多分类的概率计算公式，而lr一般用于二分类，因此二者计算概率时的公式中的分母不相同。\n\t\tii.lr也可以应用于多分类，只不过其方式是利用多个lr二分类来进行多分类。\n\t\tiii.对于ii中所说的，softmax可以进行。若待分类的类别互斥，则用softmax方法。若待分类的类别有相交，则用多分类LR，再通过投票表决（多标签分类）。\n\n* **计算效率**\n\t\t\n\n"},{"metadata":{"id":"86B72B00E8144EC98A2CC34096302A6C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# softmax使用的交叉熵损失函数\n* 对于softmax，最终得到的结果实际上是最大类所对应的概率，而这样的结果和真实的label一起计算loss时，如果还是用上述的平方损失会有一定的问题。那为什么使用交叉熵损失函数呢？\n\t\ti.想要分类结果正确，我们其实不需要预测概率完全等于标签概率。只要预测正确的概率比其他类别对应softmax值大就可以了。当使用平方损失的时候会更加严格，这样的网络会不容易收敛。\n\t\tii.因为sigmoid函数的存在，均方差对参数求偏导的过程中乘了sigmoid函数的导数，当sigmoid导数在其自变量值很大或者很小的时候其值会趋于0，所以使用均方差损失函数时，其偏导数很可能接近于0。不利于反向传播更新参数。而交叉熵损失函数对参数求偏导之后，其中的变量与sigmoid是加减的关系，因此不存在梯度消失的问题。\n\t\tiii.交叉熵其实源于相对熵（也就是KL散度），信息熵就是信息的不确定程度，信息熵越小，信息越确定。如果对于同一个随机变量X，有两个单独的概率分布P和Q，可以用相对熵来衡量这两个分布的差异。通常情况下P表示真实分布，Q表述模型预测的分布，我们希望相对熵越来越小，也就是希望P和Q分布越来越接近。而相对熵=交叉熵-信息熵。由于信息熵描述的是消除p的不确定性所需要的信息量的度量，所以其值应该是最小的，固定的。因此优化相对熵也就是优化交叉熵，所以在机器学习中使用交叉熵就可以了。\n\t\t"},{"metadata":{"id":"C01D9020699F48188BC0567A6A209D9C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 模型训练与预测\n我们使用准确率来评价模型的表现。它等于正确预测数量与总预测数量之比"},{"metadata":{"id":"AFA1D65B854D496EAE4E8CBBAFBD00A5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 获取Fashion-MNIST训练集和读取数据\n本次使用Fashion-MNIST数据集\n需要调用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构架计算机视觉模型。\ntorchvision主要有一下几部分构成\n1.torchvision.datasets:一些架子数据的函数及常用的数据集接口\n2.torchvision.models：包含常用的模型结构（含预训练模型），例如AlexNet，VGG，Resnet等\n3.torchvision.transforms:常用的图片变换，例如剪裁、旋转灯\n4.torchvision.utils:其他的一些有用的方法"},{"metadata":{"id":"B6A47E93B108428684126F9A80040DBA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting torchtext\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n\u001b[K     |████████████████████████████████| 81kB 50kB/s eta 0:00:0101\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.3.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext) (4.32.2)\nCollecting sentencepiece (from torchtext)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n\u001b[K     |████████████████████████████████| 1.0MB 30kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.17.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext) (2.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.12.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (1.25.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2019.9.11)\nInstalling collected packages: sentencepiece, torchtext\nSuccessfully installed sentencepiece-0.1.85 torchtext-0.5.0\n","name":"stdout"}],"source":"!pip install torchtext","execution_count":1},{"metadata":{"id":"8217122C835E479280F6E21ADD16D252","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n0.4.1a0+d94043a\n","name":"stdout"}],"source":"%matplotlib inline\nfrom IPython import display\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\n\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)\nprint(torchvision.__version__)","execution_count":2},{"metadata":{"id":"BF34A6AFE42B4FC49410F6DA506A79ED","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"\"\"\"\nclass torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)\n    root（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件\n    train（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。\n    download（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。\n    transform（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。\n    target_transform（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。\n\"\"\"\nmnist_train = torchvision.datasets.FashionMNIST(root='/home/kesci/input/FashionMNIST2065', train=True, download=True, transform=transforms.ToTensor())\nmnist_test = torchvision.datasets.FashionMNIST(root='/home/kesci/input/FashionMNIST2065', train=False, download=True, transform=transforms.ToTensor())","execution_count":5},{"metadata":{"id":"E3722D386AC44A2999F2288A41FAF08A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"60000 10000\n","name":"stdout"}],"source":"print(len(mnist_train), len(mnist_test))","execution_count":26},{"metadata":{"id":"3B9259E46F404DB988F90F68587A53E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"torch.Size([1, 28, 28])\n","name":"stdout"}],"source":"# 此时的输出feature是tensor\nfeature, label = mnist_train[0]\nprint(feature.shape) # CWH","execution_count":32},{"metadata":{"id":"FD6CC2D33AD04241B33320B6160286AC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 获取label函数，即数字label到文字label的转换\ndef get_fashion_mnist_labels(labels):\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]","execution_count":7},{"metadata":{"id":"04A54FF8D12F458F83CD904D54CFBA81","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 图像显示函数\ndef show_fashion_mnist(images, labels):\n    d2l.use_svg_display()\n    # 这里的_表示我们忽略（不使用）的变量\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((28, 28)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n    plt.show()","execution_count":3},{"metadata":{"id":"46BC1191231B48C3A0C241271211183C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 10 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/46BC1191231B48C3A0C241271211183C/q5mqdci94h.svg\">"},"transient":{}}],"source":"X, y = [], []\nfor i in range(10):\n    X.append(mnist_train[i][0]) # 将第i个feature加到X中\n    y.append(mnist_train[i][1]) # 将第i个label加到y中\nshow_fashion_mnist(X, get_fashion_mnist_labels(y))\n","execution_count":9},{"metadata":{"id":"216F8EC3E4314584809D44C99715A220","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 读取数据\nbatch_size = 256\nnum_workers = 4\ntrain_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\ntest_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)","execution_count":10},{"metadata":{"id":"975CA5F8070B4F0699C2C91E52629A9F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"4.77 sec\n","name":"stdout"}],"source":"start = time.time()\nfor X, y in train_iter:\n    continue\nprint('%.2f sec' % (time.time() - start))","execution_count":11},{"metadata":{"id":"C2A907D910AC4D2B815BB5D1F798AEBC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# softmax从零开始实现"},{"metadata":{"id":"EE776B6AFE1F4292888018F9AE143AC7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n0.4.1a0+d94043a\n","name":"stdout"}],"source":"import torch\nimport torchvision\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)\nprint(torchvision.__version__)","execution_count":1},{"metadata":{"id":"9D46C53F0BDE422682C64EB7DA2715C4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 读取数据集与参数初始化"},{"metadata":{"id":"933B81AF9CF5424A8EA38DAF41857247","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"batch_size = 256\n# 定义数据读取函数\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')","execution_count":2},{"metadata":{"id":"CA03AF1F5ACB4D7E84CF18F8E89A1EB1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"},"transient":{},"execution_count":3}],"source":"# 由于网络的输入的dense层，因此需要把[1, 28, 28]的tensor展开，网络的输入是28*28维的，也就是输入层应该有28*28个神经元\n# 由于网络的输出为结果，即10类，因此num_output为10\nnum_input = 784\nnum_output = 10\n\nw = torch.tensor(np.random.normal(0, 0.01, (num_input, num_output)), dtype=torch.float32)\nb  = torch.zeros(num_output, dtype=torch.float)\n\n# 模型参数初始化\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)","execution_count":3},{"metadata":{"id":"67FFDA9E3E454BE682483F22809B72B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义softmax函数以及Softmax层"},{"metadata":{"id":"03E80A5ECD714C348ABAA501DFF9FD32","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def softmax(X):\n    X_exp = X.exp()\n    # 若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。\n    # 保持原有的1维度，例如原来X是[10,1], partition 就为 [1,1] \n    partition = X_exp.sum(dim=1, keepdim=True)\n    # print(\"X size is \", X_exp, X_exp.size())\n    # print(\"partition size is \", partition, partition.size())\n    return X_exp / partition","execution_count":4},{"metadata":{"id":"B9BB3405211F4AEC81A8C87C87E53782","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def net(X):\n    return softmax(torch.mm(X.view((-1, num_input)), w) + b)","execution_count":5},{"metadata":{"id":"024E0D1E64B641CBBCB9EB9FF17AA1FE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义损失函数"},{"metadata":{"id":"F2F6DE5D88664B03BABE14354E5998DF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def cross_entropy(y_hat, y):\n    # torch.gather 函数用于从参数 t 选择性输出特定 index 的矩阵，\n    # 输出矩阵的大小跟 index 的大小是一样的，torch.gather 的dim 参数用来选择 index 作用的 axis。\n    # 这里利用y的标签选出对应交叉熵的的概率，再进行累加\n    return - torch.log(y_hat.gather(1, y.view(-1, 1)))","execution_count":6},{"metadata":{"id":"6D93A8912DF94783931C7B937C0B5327","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"tensor([[0.1000],\n        [0.5000]])"},"transient":{},"execution_count":7}],"source":"# torch中gather操作\n# 可以看出，gather的作用是这样的，index实际上是索引\n# 具体是行还是列的索引要看前面dim 的指定\n# 比如对于我们的栗子，[[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]]，\n# 指定dim=1，也就是横向，那么索引就是列号。\n# index的大小就是输出的大小，所以比如index是[[0],[2]]\n# 那么看index第一行，0列指的是0.1\n# 同理，第二行为0.5 。\n# 这样就输出为[[0.1],[0.5]]\n# 参考这样的解释看上面的输出结果，即可理解gather的含义。\n\ny_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\ny = torch.LongTensor([0, 2])\ny_hat.gather(1, y.view(-1, 1))","execution_count":7},{"metadata":{"id":"692A3EC1DE24426D9B139500F446C778","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义优化函数"},{"metadata":{"id":"9902114C87A5416E9F311888687F816C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def sgd(params, lr, batch_size):\n    for param in params:\n        param.data -= lr * param.grad / batch_size","execution_count":8},{"metadata":{"id":"4546E009146D43288048B3D70D9F6F2C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义准确率"},{"metadata":{"id":"C001430FFD154BCF8F1870C10E603864","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def accuracy(y_hat, y):\n    return (y_hat.argmax(dim=1) == y).float().mean().item()","execution_count":9},{"metadata":{"id":"E939A7C93BF44DFA8E40DFF4BEB91C86","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"0.5\n","name":"stdout"}],"source":"print(accuracy(y_hat, y))","execution_count":49},{"metadata":{"id":"8853FBD0906C4467871827157E8DAE14","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述\ndef evaluate_accuracy(data_iter, net):\n    acc_sum, n = 0.0, 0\n    for X, y in data_iter:\n        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n        n += y.shape[0]\n    return acc_sum / n","execution_count":10},{"metadata":{"id":"6E222DCE97084ACA8CB7B1DDD05192B3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 训练模型"},{"metadata":{"id":"97CD40515E6140E5B8F42D82B4EE7DA5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 1, loss 0.7837, train acc 0.751, test acc 0.796\nepoch 2, loss 0.5700, train acc 0.813, test acc 0.812\nepoch 3, loss 0.5259, train acc 0.826, test acc 0.821\nepoch 4, loss 0.5003, train acc 0.832, test acc 0.824\nepoch 5, loss 0.4850, train acc 0.837, test acc 0.829\n","name":"stdout"}],"source":"num_epochs, lr = 5, 0.1\n\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n                params=None, lr=None, optimizer=None):\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            y_hat = net(X)\n            l = loss(y_hat, y).sum()\n            \n            # 梯度清零\n            # 根据pytorch中的backward()函数的计算，当网络参量进行反馈时.\n            # 梯度是被积累的而不是被替换掉；\n            # 但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积\n            # 因此这里就需要每个batch设置一遍zero_grad 了。\n            # 其实这里还可以补充的一点是，如果不是每一个batch就清除掉原有的梯度，\n            # 而是比如说两个batch再清除掉梯度，这是一种变相提高batch_size的方法，对于计算机硬件不行，\n            # 但是batch_size可能需要设高的领域比较适合，比如目标检测模型的训练。\n\n            if optimizer is not None:\n                optimizer.zero_grad()\n            # 如果优化器未定义，那么就需要手动清零参数的梯度\n            elif params is not None and params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            \n            l.backward()\n            if optimizer is None:\n                d2l.sgd(params, lr, batch_size)\n            else:\n                optimizer.step()\n            \n            train_l_sum += l.item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\ntrain_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [w, b], lr)","execution_count":11},{"metadata":{"id":"B3BAAD3CB8A94FC3A4A48B573C410D21","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 模型预测"},{"metadata":{"id":"32D7F7E03BD74E9B8B0A9D09D1BF6B3D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 9 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/32D7F7E03BD74E9B8B0A9D09D1BF6B3D/q5my78b4ed.svg\">"},"transient":{}}],"source":"X, y = iter(test_iter).next()\n\ntrue_labels = d2l.get_fashion_mnist_labels(y.numpy())\npred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())\ntitles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n\nd2l.show_fashion_mnist(X[0:9], titles[0:9])","execution_count":12},{"metadata":{"id":"C06993D4AFFD4E9E8B3CAAEEEC4DA92B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# softmax的简洁实现"},{"metadata":{"id":"6AA96CF5E28C4B9789524D8ECED8946A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"# 加载各种包或者模块\nimport torch\nfrom torch import nn\nfrom torch.nn import init\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)","execution_count":13},{"metadata":{"id":"2936B24C5E444A69918F09D6C77CF102","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 初始化参数和获取数据"},{"metadata":{"id":"C2BD5B14BD934E9A8E7223D4F243509F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"batch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')","execution_count":14},{"metadata":{"id":"09DA2C948D1B42EC894AB73E7DF27F02","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义网络模型"},{"metadata":{"id":"50426050FEE441F6ACD03448D2588EDF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class LinearNet(nn.Module):\n    def __init__(self, num_input, num_output):\n        super(LinearNet, self).__init__()\n        self.linear = nn.Linear(num_input, num_output)\n    def forward(self, x):\n        # 把x变成batch_size*原C*W*H大小的tensor\n        y = self.linear(x.view(x.shape[0],-1))\n        return y\n\nclass FlattenLayer(nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\nfrom collections import OrderedDict\nnet = nn.Sequential(\n                    OrderedDict([('flattern', FlattenLayer()),\n                                 ('linear', nn.Linear(num_input, num_output))])\n                    )\n# 或者可以直接写成这样\nnet2 = LinearNet(num_input, num_output)","execution_count":18},{"metadata":{"id":"B76A5DC74BD34B6183DC04A242FF2F6C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 初始化模型参数"},{"metadata":{"id":"970C738EE2A04E0A85AAB797714590DF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"},"transient":{},"execution_count":19}],"source":"init.normal_(net.linear.weight, mean=0, std=0.01)\ninit.constant_(net.linear.bias, val=0)","execution_count":19},{"metadata":{"id":"C40F06753D9240D486A0B4EC30C5B364","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义损失函数"},{"metadata":{"id":"88CA564BE6DA42A78253DC62792F8BBA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"loss = nn.CrossEntropyLoss()","execution_count":20},{"metadata":{"id":"D23350490E104B498935E858944DBBFC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 定义优化函数"},{"metadata":{"id":"445B52341AA84D3D8B4A2AC6BF18EFFC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"optimizer = torch.optim.SGD(net.parameters(), lr=0.1)","execution_count":21},{"metadata":{"id":"108F7458E05C4BC0B6C5B970822C52E4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 训练\n为什么在刚开始训练的时候训练集accuracy总是比test的要高呢？\n\n\ti.训练集上的准确率是在一个epoch的过程中计算得到的，测试集上的准确率是在一个epoch结束后计算得到的，后者的模型参数更优"},{"metadata":{"id":"9AB29D3A7D724CA58F41AC73BE3CCF0A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":true,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0019, train acc 0.841, test acc 0.830\nepoch 2, loss 0.0018, train acc 0.842, test acc 0.822\nepoch 3, loss 0.0018, train acc 0.845, test acc 0.827\nepoch 4, loss 0.0018, train acc 0.848, test acc 0.834\nepoch 5, loss 0.0018, train acc 0.848, test acc 0.826\nepoch 6, loss 0.0017, train acc 0.849, test acc 0.834\nepoch 7, loss 0.0017, train acc 0.850, test acc 0.831\nepoch 8, loss 0.0017, train acc 0.851, test acc 0.837\nepoch 9, loss 0.0017, train acc 0.852, test acc 0.833\nepoch 10, loss 0.0017, train acc 0.853, test acc 0.834\nepoch 11, loss 0.0017, train acc 0.854, test acc 0.838\nepoch 12, loss 0.0017, train acc 0.855, test acc 0.834\nepoch 13, loss 0.0017, train acc 0.856, test acc 0.833\nepoch 14, loss 0.0017, train acc 0.855, test acc 0.836\nepoch 15, loss 0.0016, train acc 0.856, test acc 0.836\nepoch 16, loss 0.0016, train acc 0.858, test acc 0.839\nepoch 17, loss 0.0016, train acc 0.858, test acc 0.837\nepoch 18, loss 0.0016, train acc 0.858, test acc 0.842\nepoch 19, loss 0.0016, train acc 0.858, test acc 0.840\nepoch 20, loss 0.0016, train acc 0.859, test acc 0.839\n","name":"stdout"}],"source":"num_epochs = 20\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)","execution_count":23},{"metadata":{"id":"03B839F3CA87438E970B4C2113BA0FEE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 多层感知机"},{"metadata":{"id":"D4B88B72D6034BBB88B8F6DFB46CE5EA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 多层感知机的基本知识\n深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。\n* 隐藏层\n* 表达公式\n\t\ti.对于不含激活层的多层感知机而言，虽然神经网络引入隐藏层，却依然等价于一个单层神经网络。这样的网络结构其实设计的没什么意义。\t\t\n\t\tii.加入激活函数后的多层感知机而言，其实就是引入了非线性变换，例如对隐藏变量使用暗元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。\n* 常见激活函数：Relu激活函数，Sigmoid激活函数，tanh激活函数，pRelu函数"},{"metadata":{"id":"B042C1C510964A22895FDF04957B6D80","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 画出Relu函数及其导数函数图像"},{"metadata":{"id":"98F144250FEA4895A23ECA8F50AC0E80","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":true,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting torchtext\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n\u001b[K     |████████████████████████████████| 81kB 385kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext) (2.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.12.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.17.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.3.0)\nCollecting sentencepiece (from torchtext)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n\u001b[K     |████████████████████████████████| 1.0MB 479kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext) (4.32.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2019.9.11)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (1.25.3)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2.8)\nInstalling collected packages: sentencepiece, torchtext\nSuccessfully installed sentencepiece-0.1.85 torchtext-0.5.0\n","name":"stdout"}],"source":"!pip install torchtext","execution_count":3},{"metadata":{"id":"6B22CC3EA34743088D0BDBAF2BDA8F8A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\nprint(torch.__version__)","execution_count":4},{"metadata":{"id":"6DF3220983FE4917B82B98EACF24C4DE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def xyplot(x_vals, y_vals, name):\n    # d2l.set_figsize(figsize=(5, 2.5))\n    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())\n    plt.xlabel('x')\n    plt.ylabel(name + '(x)')","execution_count":7},{"metadata":{"id":"2D4170EB256D41D4AFE53D5345CA1F59","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def xyplot(x_vals, y_vals, name):\n    # d2l.set_figsize(figsize=(5, 2.5))\n    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())\n    plt.xlabel('x')\n    plt.ylabel(name + '(x)')","execution_count":8},{"metadata":{"id":"7FE71FB29F0E498C820F36A5AFDAD629","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/7FE71FB29F0E498C820F36A5AFDAD629/q5nfvd4roe.png\">"},"transient":{}}],"source":"x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\ny = x.relu()\nxyplot(x, y, 'relu')","execution_count":9},{"metadata":{"id":"4D1CECD0EA804A39923C8F61D9561E49","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/4D1CECD0EA804A39923C8F61D9561E49/q5nfwgnszi.png\">"},"transient":{}}],"source":"y.sum().backward()\nxyplot(x, x.grad, 'grad of relu')","execution_count":10},{"metadata":{"id":"9FB17BEA0DD0418ABBCA0096539A2C75","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"\n可以看到上述图像，Relu求导后不会产生所谓sigmoid的梯度消失的情况，因为不存在只有数值两极分化是才有梯度，而其他时候没有梯度的情况。\n但是Relu会使得部分神经元死亡，即当某一神经元线性映射的值为负时，其对应的梯度为零，因此某种程度上，Relu依然会导致梯度消失的情况，从而使得训练困难。"},{"metadata":{"id":"67108515B9AD422E8CA7027609AC6E8D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Sigmoid函数图像以其导数函数图像"},{"metadata":{"id":"615B48BC9D374262A7B42F891E71F34D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/615B48BC9D374262A7B42F891E71F34D/q5ng5rqa6f.png\">"},"transient":{}}],"source":"y = x.sigmoid()\nxyplot(x, y, 'sigmoid')","execution_count":11},{"metadata":{"id":"4B04C8EA3A2E49C7A73EEBFF4E2089C2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/4B04C8EA3A2E49C7A73EEBFF4E2089C2/q5ng62qu3e.png\">"},"transient":{}}],"source":"x.grad.zero_()\ny.sum().backward()\nxyplot(x, x.grad, 'grad of sigmoid')","execution_count":12},{"metadata":{"id":"6C6A051B58D347C49A4D0F80A82A6B82","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。\n与此同时sigmoid函数是没有负值的，这对这个函数有一定的限制。"},{"metadata":{"id":"A8E8F98FB0004DA788D34318992DFF56","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 正切函数"},{"metadata":{"id":"9094EB8666974FAB865D9169697B2268","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9094EB8666974FAB865D9169697B2268/q5ngbshr9v.png\">"},"transient":{}}],"source":"y = x.tanh()\nxyplot(x, y, 'tanh')","execution_count":14},{"metadata":{"id":"E2CCF3E87BB44BD6AA7620A55148CB82","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/E2CCF3E87BB44BD6AA7620A55148CB82/q5ngby4c58.png\">"},"transient":{}}],"source":"x.grad.zero_()\ny.sum().backward()\nxyplot(x, x.grad, 'grad of tanh')","execution_count":15},{"metadata":{"id":"1737EF10E3BC4E8F88F6C0AB3D52AE16","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 关于激活函数的选择\n\nSigmoid函数以及它们的联合通常在分类器的中有更好的效果\n由于梯度崩塌的问题，在某些时候需要避免使用Sigmoid和Tanh激活函数\nReLU函数是一种常见的激活函数，在目前使用是最多的\n如果遇到了一些死的神经元，我们可以使用Leaky ReLU函数\n记住，ReLU永远只在隐藏层中使用\n根据经验，我们一般可以从ReLU激活函数开始，但是如果ReLU不能很好的解决问题，再去尝试其他的激活函数\n"},{"metadata":{"id":"5E592C385C294A778718194D7539DED7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# todo \n# 等之后补充过拟合以及梯度爆炸的相关代码"},{"metadata":{"id":"23CB053E0BC7443BA693EA1A02BCA39B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 文本处理\n文本是一类序列数据，一篇文章可以看做是字符或是单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：\n1.读入文本\n2.分词\n3.建立字典，将每个词映射到一个唯一的索引\n4.将文本从词的序列转换为索引的序列，方便输入模型"},{"metadata":{"id":"F2CC4CE7CC5F42068A5F9A36685220FE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 读入文本"},{"metadata":{"id":"AC0C256C98DB4268A146A0C5CABF8BC1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"text = 'What is you problem?'","execution_count":2},{"metadata":{"id":"EE0A499FDC4340468E22A663A9C17638","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'what is you problem '"},"transient":{},"execution_count":6}],"source":"re.sub('[^a-z]+', ' ', text.strip().lower())","execution_count":6},{"metadata":{"id":"5D0F6899C6C944B5B2C1E239101D686E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"# sentence 3221\n","name":"stdout"}],"source":"import collections\nimport re\n\ndef read_time_machine():\n    with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f:\n        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n        # 这里的正则表达式表示的是除了小写字母外的所有字符都转换为‘ ’空格\n    return lines\n\nlines = read_time_machine()\n# lines 是一个由每个句子组成的list\nprint('# sentence %d' % len(lines))","execution_count":2},{"metadata":{"id":"2CDE57A573BF480E8F2F7EFE351A6819","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 分词 \n将lines中的句子通过‘’进行分词，得到关于词的list"},{"metadata":{"id":"23C3C35FA27A41AEBB0894A3FF518D0D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], [''], ['']]\n","name":"stdout"}],"source":"def tokenize(sentences, token='word'):\n    \"\"\"Split sentenses into word into word or char tokens\"\"\"\n    if token == 'word':\n        return [sentence.split(' ') for sentence in sentences]\n    elif token == 'char':\n        return [list(sentence) for sentence in sentences]\n    else:\n        print('ERROR: unkown token type ' + token)\n\ntokens = tokenize(lines)\nprint(tokens[0:3])","execution_count":3},{"metadata":{"id":"E0E9F5E6CFAD429181E04994FE7E1E47","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 建立字典\n进行词频过滤，之后进行一个词到数字下标的映射"},{"metadata":{"id":"F998DE0A82EE4148810C7AE94606F0CB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def count_corpus(sentences):\n    tokens = [tk for st in sentences for tk in st]\n    return collections.Counter(tokens)\n\nclass Vocab(object):\n    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n        counter = count_corpus(tokens)\n        self.token_freq = list(counter.items())\n        self.idx_to_token = []\n        if use_special_tokens:\n            # 因为句子有长短，所以用self.pad及逆行补全\n            # 为了区分句子与句子，用bos标志和eos标志来进行句子的划分\n            # unk表示unknown\n            self.pad, self.bos, self.eos, self.unk = (0,1,2,3)\n            self.idx_to_token += ['', '', '', '']\n        else:\n            self.unk = 0\n            self.idx_to_token += ['']\n        # 去除频率小于min_freq并且去重\n        self.idx_to_token += [token for token, freq in self.token_freq\n                        if freq >= min_freq and token not in self.idx_to_token ]\n        self.token_to_idx = dict()\n        for idx, token in enumerate(self.idx_to_token):\n            self.token_to_idx[token] = idx\n    \n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n        ","execution_count":10},{"metadata":{"id":"D3264773CDB7483683F1AEBC8AE21002","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n","name":"stdout"}],"source":"vocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[0:10])","execution_count":11},{"metadata":{"id":"5DB4F6560F1F4DA59D0A550CA4322DB5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 将词转为索引\n使用字典，我们可以将原文本中的句子从单词序列转换为索引序列"},{"metadata":{"id":"388B496E2F4B4C188B976502881CDDE4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\nindices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\nwords: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\nindices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n","name":"stdout"}],"source":"for i in range(8, 10):\n    print('words:', tokens[i])\n    print('indices:', vocab[tokens[i]])","execution_count":12},{"metadata":{"id":"1A03B5D7864F43B6A9CF4B3984F83738","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 我们前面介绍的分词方式非常简单，它至少有以下几个缺点:\n\n1. 标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了\n2. 类似“shouldn't\", \"doesn't\"这样的词会被错误地处理\n3. 类似\"Mr.\", \"Dr.\"这样的词会被错误地处理\n\n我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：[spaCy](https://spacy.io/)和[NLTK](https://www.nltk.org/)。"},{"metadata":{"id":"7CDA931EDA2C472A99B5F468E8B749C6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"text = \"Mr. Chen doesn't agree with my suggestion.\"","execution_count":13},{"metadata":{"id":"3FFF6F2E12394BE286658906E30DB67C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n","name":"stdout"}],"source":"import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nprint([token.text for token in doc])","execution_count":14},{"metadata":{"id":"C520E761A6CD423B8C7B4D21F2B9D381","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n","name":"stdout"}],"source":"from nltk.tokenize import word_tokenize\nfrom nltk import data\ndata.path.append('/home/kesci/input/nltk_data3784/nltk_data')\nprint(word_tokenize(text))","execution_count":15},{"metadata":{"id":"1660D575D12B49BC84598F1DC164ED1D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"\n\n# 语言模型与数据集\n一段自然语言文本可以看做是一个离散时间序列，给定一个长度为T的词的序列w1,w2,...,wt，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：\n本节我们介绍基于统计的语言模型，主要是n元语法。在后续内容中，我们将会介绍基于神经网络的语言模型。\n**语言模型主要是以下模型**\n其中需要求解的是各个概率（这也是参数）\n$$\n\n\\begin{align*}\nP(w_1, w_2, \\ldots, w_T)\n&= \\prod_{t=1}^T P(w_t \\mid w_1, \\ldots, w_{t-1})\\\\\n&= P(w_1)P(w_2 \\mid w_1) \\cdots P(w_T \\mid w_1w_2\\cdots w_{T-1})\n\\end{align*}\n\n$$\n**n-gram**\n这里n-gram主要是依赖于马尔科夫条件假设，即当前词出现的概率至于前面的n个词有关系。\n由于句子可以天然的看做是马尔科夫链，因此对于选择设计当前词语前面出现的1/2/3...n个词有关变成了1元语法/2元语法/3元语法。\n于是上述的语言模型就简化成了如下式子\n$$\n\n\\begin{aligned}\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\\\\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\\\\nP(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) .\n\\end{aligned}\n\n$$\n\n指的一提的是，当n比较小的时候，n元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你先走”和“你走先”的概率屎一样的。然而，当n比较大的时候，n元语法需要计算并存储大量的词频和多词相邻频率。\n\n**n元语法的缺陷在于：**\n\t\t\n\ti.参数空间过大。\n\t对照模型公式可以看出，需要计算不同词相继出现的各种频率。对于一个含有非常多词的词库来说，各种参数(概率)是非常非常大的。\n\tii.数据稀疏。\n\t因为词出现的频率这一概率值是非常小的，与此同时，对于计算P(w_3 \\mid w_1, w_2)这样的参数某些词组出现的概率会非常小。数据会非常稀疏。\n\n\n\n\n"},{"metadata":{"id":"8D2E57AD4E95438283ACA16DE5A7F390","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 语言模型数据集"},{"metadata":{"id":"01EA2C5EDBBB423A94BA4E828DA8A92B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 读取数据集"},{"metadata":{"id":"4297331A40CA47919C00D1922A4F859B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"63282\n<class 'str'>\n","name":"stdout"}],"source":"with open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n    corpus_chars = f.read()\n\nprint(len(corpus_chars))\n# print(corpus_chars)\n# 用' '替代换行符，用' ' 代替空格\ncorpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\ncorpus_chars = corpus_chars[:10000]\nprint(type(corpus_chars))","execution_count":20},{"metadata":{"id":"83F69F9E2F2D4CB9A028A9B7F77AAA0F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 建立字符索引"},{"metadata":{"id":"1E31E192E2F049709E7C182931A0ADD1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","source":"# 去重,建立字符到索引的映射idx_to_char\nidx_to_char = list(set(corpus_chars))\n# 建立索引到字符的映射corpus_indices\nchar_to_idx = {idx:char for char, idx in enumerate(idx_to_char)}\nvocab_size = len(char_to_idx)\nprint(vocab_size)\n\ncorpus_indices = [char_to_idx[char]  for char in corpus_chars]\nsample = corpus_indices[:20]\nprint('chars:', ''.join([idx_to_char[idx] for idx in sample]))\nprint('indices:', sample)","execution_count":24,"outputs":[]},{"metadata":{"id":"B2968CCACEC14A0D8571D8945472F629","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 时间序列的采样\n对于时间序列的采样，进行的操作是对于一句话，选取一定的时间步数，之后再句子中选取连续的时间步数的词组char作为训练集的样本（也就是下面的X），而其对应的char向后退移动一步作为训练样本对应的label（也就是下面的Y）。\n举个例子：\n现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：\n* $X$：“想要有直升”，$Y$：“要有直升机”\n* $X$：“要有直升机”，$Y$：“有直升机，”\n* $X$：“有直升机，”，$Y$：“直升机，想”\n* ...\n* $X$：“要和你飞到”，$Y$：“和你飞到宇”\n* $X$：“和你飞到宇”，$Y$：“你飞到宇宙”\n* $X$：“你飞到宇宙”，$Y$：“飞到宇宙去”\n可以看到，如果序列的长度为T，时间步数为n，那么一共有T-n个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。"},{"metadata":{"id":"1C7E072A44414EA28913EBE0991C380A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 随机采样\n下面的代码每次从数据里随机采样一个小批量。其中批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。\n在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。**即先切片再随机抽样。**"},{"metadata":{"id":"752F08FAC2444E21AC65BDDAC98A14B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import torch\nimport random\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n-1个字符\n    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n    # 得到可取到每个时间步数的字符组的下标\n    example_indices = [i * batch_size for i in range(num_examples)]\n    random.shuffle(example_indices)\n    \n    def _data(i):\n        return corpus_indices[i: i+num_steps]\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n    for i in range(0, num_examples, batch_size):\n        # 每次选出batch_size个样本\n        batch_size_indices = example_indices[i:i+batch_size]\n        X = [_data(j) for j in batch_size_indices]\n        Y = [_data(j+1) for j in batch_size_indices]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n        ","execution_count":29},{"metadata":{"id":"C255FF5681AC4FCE85F2EB207640A266","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":true,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[ 6,  7,  8,  9, 10, 11],\n        [ 2,  3,  4,  5,  6,  7]]) \nY: tensor([[ 7,  8,  9, 10, 11, 12],\n        [ 3,  4,  5,  6,  7,  8]]) \n\nX:  tensor([[4, 5, 6, 7, 8, 9],\n        [0, 1, 2, 3, 4, 5]]) \nY: tensor([[ 5,  6,  7,  8,  9, 10],\n        [ 1,  2,  3,  4,  5,  6]]) \n\n","name":"stdout"}],"source":"my_seq = list(range(30))\nfor X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":30},{"metadata":{"id":"D1DAAC2B672C4DBB9D097DB3A50ADE65","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 相邻采样\n在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。**即当前batch是上一个batch是相邻样本，先切分，后按顺序采样。**"},{"metadata":{"id":"FC13B8AACE9044388048C8D996B67023","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    corpus_len = len(corpus_indices) // batch_size * batch_size\n    corpus_indices = corpus_indices[:corpus_len]\n    indices = torch.tensor(corpus_indices, device=device)\n    # resize成batch_size行\n    indices = indices.view(batch_size, -1)\n    batch_num = (indices.shape[1] - 1) // num_steps\n    for i in range(batch_num):\n        i = i * num_steps\n        # 行全取，每次取num_steps列\n        X = indices[:,i:i+num_steps]\n        Y = indices[:,i+1:i+num_steps+1]\n        yield X, Y","execution_count":31}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}